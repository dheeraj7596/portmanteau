{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "import numpy as np\n",
    "from sentence_getter import SentenceGetter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras import optimizers\n",
    "import kenlm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pylab as pl\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.metrics import r2_score\n",
    "import heapq\n",
    "from collections import defaultdict \n",
    "\n",
    "def get_model(max_len, n_words, n_tags, embedding_mat):\n",
    "    input = Input(shape=(max_len,))\n",
    "    model = Embedding(input_dim=n_words, weights=[embedding_mat], output_dim=50, input_length=max_len)(input)\n",
    "    model = Dropout(0.1)(model)\n",
    "    model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "    out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)  # softmax output layer\n",
    "    model = Model(input, out)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_embedding_matrix(embeddings_path, word2idx):\n",
    "    embedding_vectors = {}\n",
    "    with open(embeddings_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line_split = line.strip().split(\" \")\n",
    "            vec = np.array(line_split[1:], dtype=float)\n",
    "            char = line_split[0]\n",
    "            embedding_vectors[char] = vec\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word2idx), 50))\n",
    "    for char in word2idx:\n",
    "        embedding_vector = embedding_vectors.get(char)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word2idx[char]] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def get_word(X, y, words, tags):\n",
    "    ans = \"\"\n",
    "    for i, ch in enumerate(X):\n",
    "        if tags[y[i]] == \"C\":\n",
    "            ans += words[ch]\n",
    "    return ans\n",
    "\n",
    "def get_word2(word,tag_seq, words):\n",
    "    ans = \"\"\n",
    "    for i in range(len(word)):\n",
    "        if tag_seq[i]=='C':\n",
    "            ans+=words[word[i]]\n",
    "    return ans\n",
    "\n",
    "def score_candidate_length(c, example,length_model):\n",
    "    import scipy.stats\n",
    "    needed_length, needed_length_std=length_model.predict(np.array([[len(example)]]), return_std=True)\n",
    "    needed_length=needed_length[0]\n",
    "    needed_length_std=needed_length_std[0]\n",
    "    clength=len(c)\n",
    "    #print \"NL:{} CL:{}\".format(needed_length, clength)\n",
    "    return scipy.stats.norm.logpdf(clength, loc=needed_length, scale=needed_length_std)-scipy.stats.norm.logpdf(needed_length, loc=needed_length, scale=needed_length_std)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "def updatestr(s,i,ch):  \n",
    "    list1 = list(s)\n",
    "    list1[i] = ch\n",
    "    str1 = ''.join(list1)\n",
    "    return str1\n",
    "\n",
    "def getTopk(m,k, r_mapping):\n",
    "    mapping = {}\n",
    "    for tag in r_mapping:\n",
    "        mapping[r_mapping[tag]] = tag\n",
    "#     mapping = {0:'C',1:'D',2:'O'}\n",
    "#     r_mapping = {'C':0, 'D':1, 'O':2}\n",
    "    best_seq = \"\"\n",
    "    best_prob = 1.0\n",
    "    best_idx = np.argmax(m, axis=-1)\n",
    "    for i in range(30):\n",
    "        best_seq += mapping[best_idx[i]]\n",
    "        best_prob *= m[i][best_idx[i]]\n",
    "    heap = [(-1*best_prob,best_seq)]\n",
    "    heapq.heapify(heap)\n",
    "    \n",
    "    result = []\n",
    "    added = set()\n",
    "    while k>0:\n",
    "        top = heapq.heappop(heap)\n",
    "        result += [(top[0]*-1,top[1])]\n",
    "        added.add(top[1])\n",
    "        k-=1\n",
    "        prob = -1*top[0]\n",
    "        seq = top[1]\n",
    "        curr_prob = prob\n",
    "        curr_seq = seq\n",
    "        for i in range(30):\n",
    "            for j in range(3):\n",
    "                curr_seq = updatestr(curr_seq,i,mapping[j])\n",
    "                if curr_seq in added:\n",
    "                    continue\n",
    "                curr_prob = prob*m[i][j]/m[i][r_mapping[seq[i]]]\n",
    "                heapq.heappush(heap,(-1*curr_prob,curr_seq))\n",
    "                curr_seq = seq\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence after  2  iterations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=True, copy_X=True,\n",
       "              fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
       "              normalize=False, tol=0.001, verbose=True)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_len(row):\n",
    "    from collections import Counter\n",
    "    return Counter(list(row))['C']\n",
    "df = pd.read_csv('./data/components-blends-knight.csv',sep='\\t',index_col=0)\n",
    "df[\"slen\"]=df.source.apply(len)\n",
    "df[\"tlen\"]=df.target.apply(get_len)\n",
    "df[\"ratio\"]=df[\"slen\"]/df[\"tlen\"]\n",
    "len_model = BayesianRidge(verbose=True, compute_score=True)\n",
    "X=df[\"slen\"].values.reshape(-1,1)\n",
    "y=df[\"tlen\"].values\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y)\n",
    "len_model.fit(X_train, y_train)\n",
    "# r2_score(y_test,len_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = 3\n",
    "X_tr = []\n",
    "X_te = []\n",
    "y_tr = [] \n",
    "y_te = []\n",
    "\n",
    "for i in range(kfold):\n",
    "    data = pickle.load(open(\"./data/df_lstm.pkl\", \"rb\"))\n",
    "\n",
    "    embeddings_path = \"./data/pretrained_char_emb.txt\"\n",
    "\n",
    "    words = list(set(data[\"Word\"].values))\n",
    "    words.append(\"$\")\n",
    "    n_words = len(words)\n",
    "    tags = list(set(data[\"Tag\"].values))\n",
    "    tags.append(\"O\")\n",
    "    n_tags = len(tags)\n",
    "    getter = SentenceGetter(data)\n",
    "    sentences = getter.sentences\n",
    "    max_len = 30\n",
    "    word2idx = {w: i for i, w in enumerate(words)}\n",
    "    tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "\n",
    "    embedding_mat = get_embedding_matrix(embeddings_path, word2idx)\n",
    "\n",
    "    X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "    X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=n_words - 1)\n",
    "    y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
    "    y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "    y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
    "    X_tr_t, X_te_t, y_tr_t, y_te_t = train_test_split(X, y, test_size=0.1)\n",
    "    X_tr += [X_tr_t]\n",
    "    X_te += [X_te_t]\n",
    "    y_tr += [y_tr_t]\n",
    "    y_te += [y_te_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model  1  trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model  2  trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model  3  trained.\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "lmodel = kenlm.Model('./data/wordlist_english_filtered_threshold100-kenlm.arpa')\n",
    "for i in range(kfold):\n",
    "    model = get_model(max_len, n_words, n_tags, embedding_mat)\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"accuracy\"])\n",
    "    history = model.fit(X_tr[i], np.array(y_tr[i]), batch_size=32, epochs=10, validation_split=0.1, verbose=0)\n",
    "    models += [model]\n",
    "    print(\"model \",i+1,\" trained.\")\n",
    "# from keras.models import load_model\n",
    "\n",
    "# models += [load_model(\"./keras_jupyper.h5\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEditDistance(model,idx,l1,l2,l3):\n",
    "    preds = []\n",
    "    true = []\n",
    "    for i, test in enumerate(X_te[idx]):\n",
    "        p = model.predict(np.array([X_te[idx][i]]))\n",
    "        t = y_te[idx][i]\n",
    "        predictions = getTopk(p[0],10, tag2idx)\n",
    "#         print(predictions)\n",
    "        candidates = [get_word2(X_te[idx][i],d[1], words) for d in predictions]\n",
    "        m_scores=[lmodel.score(\" \".join(c))/(float(len(\" \".join(c))**1)) for c in candidates]\n",
    "        input_len = [len_model.predict([[p[1].index('O')]])[0] if 'O' in p[1] else 30 for p in predictions]\n",
    "        lstm_len = [(p[1].index('O') - p[1].count('D')) if 'O' in p[1] else 30 for p in predictions]\n",
    "        len_score = [1/(1+(abs(i-l))) for l,i in zip(lstm_len,input_len)]\n",
    "        for j in range(len(m_scores)):\n",
    "#             print(m_scores[j]/l1,predictions[j][0]*l2,len_score[j]/l3)\n",
    "            m_scores[j]= m_scores[j]/l1 +  predictions[j][0]*l2 + len_score[j]/l3\n",
    "        \n",
    "        max_idx=-1\n",
    "        max_val = -99999\n",
    "        for ele in enumerate(m_scores):\n",
    "            if ele[1]>max_val:\n",
    "                max_val = ele[1]\n",
    "                max_idx = ele[0]\n",
    "        preds.append(candidates[max_idx])\n",
    "    #     p = np.argmax(p, axis=-1)\n",
    "        t = np.argmax(t, axis=-1)\n",
    "    #     preds.append(get_word(X_te[i], p[0], words, tags))\n",
    "        true.append(get_word(X_te[idx][i], t, words, tags))\n",
    "\n",
    "    distance = 0\n",
    "    for i,word in enumerate(true):\n",
    "        distance += Levenshtein.distance(word, preds[i])\n",
    "    acc = distance / len(preds)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch(model,i):\n",
    "    best_ed = 999\n",
    "    l1 = np.arange(0,1,0.1)\n",
    "    l2 = np.arange(0,1,0.1)\n",
    "    for ele1 in l1:\n",
    "        for ele2 in l2:\n",
    "            ed = getEditDistance(model,i,ele1,ele2)\n",
    "            if ed<best_ed:\n",
    "                best_ed = ed\n",
    "    return best_ed\n",
    "avg_edit = 0\n",
    "for i in range(kfold):\n",
    "#     res = getEditDistance(models[i],i)\n",
    "    res = gridsearch(models[i],i)\n",
    "    avg_edit += res\n",
    "    \n",
    "avg_edit/kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8445945945945945"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getEditDistance(models[0],0,8,1,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
